{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recession effect on Housing Prices from University Towns\n",
    "\n",
    "This project is about examining the Hypothesis, **\"The housing price from homes across university towns were less effected by the Great Recession\".**\n",
    "\n",
    "Glossary terms/Definitions used in this assignment:\n",
    "* A _quarter_ is a specific three month period.\n",
    "* A _recession_ is defined as the period which starts with two consecutive quarters of GDP decline, and ending with two consecutive quarters of GDP growth.\n",
    "* A _recession bottom_ is the quarter within a recession which had the lowest GDP.\n",
    "* A _university town_ is a city which has a high percentage of university students compared to the total population of the city.\n",
    "\n",
    "The following data files are used for this project:\n",
    "* ```City_Zhvi_AllHomes.csv```, obtained from [Zillow research data site](http://files.zillowstatic.com/research/public/City/City_Zhvi_AllHomes.csv), contains US median house sale prices at city level.\n",
    "* A list of university towns in the United States can be obtained by webscraping the [Wikipedia page](https://en.wikipedia.org/wiki/List_of_college_towns#College_towns_in_the_United_States) on college towns which has been copied and pasted into the file ```university_towns.txt```.\n",
    "* GDP over time of the United States in current US dollars, in quarterly intervals, can be obtained from [Bureau of Economic Analysis, US Department of Commerce](http://www.bea.gov/national/index.htm#gdp) which has been copied and pasted into the file ```ugdplev.xls```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas which is high-performance, easy-to-use data structures and data analysis framework\n",
    "import numpy as np # import numpy which is fundamental package for scientific computing with Python\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data_ds/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to map state names to two letter acronyms\n",
    "state_acronyms = {'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', \n",
    "                  'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', \n",
    "                  'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia',\n",
    "                  'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', \n",
    "                  'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam',\n",
    "                  'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', \n",
    "                  'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', \n",
    "                  'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', \n",
    "                  'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', \n",
    "                  'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', \n",
    "                  'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_university_towns_list():\n",
    "    '''Returns a DataFrame of towns and the states they are in from the \n",
    "    university_towns.txt list.'''\n",
    "  \n",
    "    data = pd.read_table(data_dir + 'university_towns.txt', header=None) # read university_towns.txt which contains a list of university towns in the United States and store it in a dataframe\n",
    "    data['Cleaned Data'] = data[0].apply(lambda x: x.split(\n",
    "    '(')[0].strip() if x.count('(') > 0 else x) # removing every character from \"(\" to the end. \n",
    "    data['RegionName'] = data['Cleaned Data'].apply(lambda x: x if x.count('[edit]') == 0 else np.NaN) #  replacing every state (ending with \"[edit]\") with NaN in a new column, \"Region name\",for easier selection of counties \n",
    "    data[\"State\"] = 0 # initialise state column with 0\n",
    "    state = None\n",
    "    for index, entry in enumerate(data['Cleaned Data']): #assigning state names to regions\n",
    "        if entry.count('[edit]') > 0:\n",
    "           state = entry.split('[')[0].strip()\n",
    "           data[\"State\"][index] = state\n",
    "        else:\n",
    "           data[\"State\"][index] = state\n",
    "    data = data.drop(0, axis=1) #droping unwanted columns\n",
    "    data = data.dropna() # dropping rows with null values\n",
    "    data.index = list(range(len(data))) # reindexing from 0 to length of the datarfame\n",
    "    columns_to_keep = ['State',\n",
    "                       'RegionName'] \n",
    "    data = data[columns_to_keep] # keeping only columns of interest\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>RegionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Auburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Jacksonville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Livingston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Montevallo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Troy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Tuscaloosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Tuskegee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Fairbanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Flagstaff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     State    RegionName\n",
       "0  Alabama        Auburn\n",
       "1  Alabama      Florence\n",
       "2  Alabama  Jacksonville\n",
       "3  Alabama    Livingston\n",
       "4  Alabama    Montevallo\n",
       "5  Alabama          Troy\n",
       "6  Alabama    Tuscaloosa\n",
       "7  Alabama      Tuskegee\n",
       "8   Alaska     Fairbanks\n",
       "9  Arizona     Flagstaff"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_university_towns_list().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recession_start_quarter():\n",
    "    '''Returns the year and quarter of the recession start time.'''\n",
    "    \n",
    "    GDP_over_time = pd.ExcelFile(data_dir + 'gdplev.xls') # read university_towns.txt which contains quarterly GDP over time in the United States and store it in a dataframe with header excluded\n",
    "    GDP_over_time = GDP_over_time.parse(GDP_over_time.sheet_names[0]) # parse the first sheet of the dataframe\n",
    "    del GDP_over_time[GDP_over_time.columns[3]], GDP_over_time[GDP_over_time.columns[-1]] # delete unwanted columns\n",
    "    GDP_over_time.drop(GDP_over_time.index[:7],inplace=True) # drop unwanted rows\n",
    "    GDP_over_time = GDP_over_time.rename(index=str, columns={'Current-Dollar and \"Real\" Gross Domestic Product':'Year','Unnamed: 1':'GDP in billions of current dollars 1','Unnamed: 2':'GDP in billions of chained 2009 dollars 1','Unnamed: 4':'Quarter','Unnamed: 5':'GDP in billions of current dollars 2','Unnamed: 6':'GDP in billions of chained 2009 dollars 2'}) # renaming column headers\n",
    "    millennium_first_quarter = GDP_over_time[GDP_over_time['Quarter']=='2000q1'].index[0] # extracting index of millennium start quarter\n",
    "    GDP_over_time = GDP_over_time.loc[millennium_first_quarter:] # extracting the dataframe from millennium start quarter\n",
    "    del GDP_over_time['Year'], GDP_over_time['GDP in billions of current dollars 1'], GDP_over_time['GDP in billions of chained 2009 dollars 1'] # deleting unwanted columns\n",
    "    GDP_over_time.index = list(range(len(GDP_over_time))) # reindexing from 0 to length of the datarfame\n",
    "    Quarterly_GDP = 0.0\n",
    "    GDP_Decline_Counter = 0\n",
    "    recession_start_time = None\n",
    "    for index, entry in enumerate(GDP_over_time['GDP in billions of current dollars 2']): # extracting recession start quarter\n",
    "        if entry > Quarterly_GDP:\n",
    "            Quarterly_GDP = entry\n",
    "        elif GDP_Decline_Counter==2:\n",
    "            Quarterly_GDP = entry\n",
    "            recession_start_time = GDP_over_time['Quarter'][int(index)-3]\n",
    "        else:\n",
    "            Quarterly_GDP = entry\n",
    "            GDP_Decline_Counter += 1\n",
    "    return recession_start_time # returning recession start quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008q3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recession_start_quarter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recession_end_quarter():\n",
    "    '''Returns the year and quarter of the recession end time.'''\n",
    "    \n",
    "    GDP_over_time = pd.ExcelFile(data_dir + 'gdplev.xls') # read university_towns.txt which contains quarterly GDP over time in the United States and store it in a dataframe with header excluded\n",
    "    GDP_over_time = GDP_over_time.parse(GDP_over_time.sheet_names[0]) # parse the first sheet of the dataframe\n",
    "    del GDP_over_time[GDP_over_time.columns[3]], GDP_over_time[GDP_over_time.columns[-1]] # delete unwanted columns\n",
    "    GDP_over_time.drop(GDP_over_time.index[:7],inplace=True) # drop unwanted rows\n",
    "    GDP_over_time = GDP_over_time.rename(index=str, columns={'Current-Dollar and \"Real\" Gross Domestic Product':'Year','Unnamed: 1':'GDP in billions of current dollars 1','Unnamed: 2':'GDP in billions of chained 2009 dollars 1','Unnamed: 4':'Quarter','Unnamed: 5':'GDP in billions of current dollars 2','Unnamed: 6':'GDP in billions of chained 2009 dollars 2'}) # renaming column headers\n",
    "    millennium_first_quarter = GDP_over_time[GDP_over_time['Quarter']=='2000q1'].index[0] # extracting index of millennium start quarter\n",
    "    GDP_over_time = GDP_over_time.loc[millennium_first_quarter:] # extracting the dataframe from millennium start quarter\n",
    "    del GDP_over_time['Year'], GDP_over_time['GDP in billions of current dollars 1'], GDP_over_time['GDP in billions of chained 2009 dollars 1'] # deleting unwanted columns\n",
    "    GDP_over_time.index = list(range(len(GDP_over_time))) # reindexing from 0 to length of the datarfame\n",
    "    Quarterly_GDP = 0.0\n",
    "    GDP_Decline_Counter = 0\n",
    "    recession_start_time = None\n",
    "    for index, entry in enumerate(GDP_over_time['GDP in billions of current dollars 2']): # extracting recession end quarter\n",
    "        if entry > Quarterly_GDP:\n",
    "            Quarterly_GDP = entry\n",
    "        elif GDP_Decline_Counter==2:\n",
    "            Quarterly_GDP = entry\n",
    "            recession_start_time = GDP_over_time['Quarter'][int(index)-3]\n",
    "        else:\n",
    "            Quarterly_GDP = entry\n",
    "            GDP_Decline_Counter += 1\n",
    "    \n",
    "    recession_start_quarter = GDP_over_time[GDP_over_time['Quarter']==recession_start_time].index[0]\n",
    "    GDP_over_time_revised = GDP_over_time.loc[recession_start_quarter:]\n",
    "    GDP_over_time_revised.index = list(range(len(GDP_over_time_revised)))\n",
    "    \n",
    "    Quarterly_GDP = GDP_over_time_revised['GDP in billions of current dollars 2'][0]\n",
    "    GDP_Growth_Counter = 0\n",
    "    recession_end_time = 0\n",
    "    for index, entry in enumerate(GDP_over_time_revised['GDP in billions of current dollars 2']):\n",
    "        if GDP_Growth_Counter == 2:\n",
    "            Quarterly_GDP = entry\n",
    "            recession_end_time = GDP_over_time_revised['Quarter'][int(index-1)]\n",
    "            break\n",
    "        elif entry > Quarterly_GDP:\n",
    "            Quarterly_GDP = entry\n",
    "            GDP_Growth_Counter += 1\n",
    "        else:\n",
    "            Quarterly_GDP = entry\n",
    "        \n",
    "    return recession_end_time # returning recession end quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recession_end_quarter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recession_bottom_quarter():\n",
    "    '''Returns the year and quarter of the recession bottom time.'''\n",
    "    \n",
    "    GDP_over_time = pd.ExcelFile(data_dir + 'gdplev.xls') # read university_towns.txt which contains quarterly GDP over time in the United States and store it in a dataframe with header excluded\n",
    "    GDP_over_time = GDP_over_time.parse(GDP_over_time.sheet_names[0]) # parse the first sheet of the dataframe\n",
    "    del GDP_over_time[GDP_over_time.columns[3]], GDP_over_time[GDP_over_time.columns[-1]] # delete unwanted columns\n",
    "    GDP_over_time.drop(GDP_over_time.index[:7],inplace=True) # drop unwanted rows\n",
    "    GDP_over_time = GDP_over_time.rename(index=str, columns={'Current-Dollar and \"Real\" Gross Domestic Product':'Year','Unnamed: 1':'GDP in billions of current dollars 1','Unnamed: 2':'GDP in billions of chained 2009 dollars 1','Unnamed: 4':'Quarter','Unnamed: 5':'GDP in billions of current dollars 2','Unnamed: 6':'GDP in billions of chained 2009 dollars 2'}) # renaming column headers\n",
    "    millennium_first_quarter = GDP_over_time[GDP_over_time['Quarter']=='2000q1'].index[0] # extracting index of millennium start quarter\n",
    "    GDP_over_time = GDP_over_time.loc[millennium_first_quarter:] # extracting the dataframe from millennium start quarter\n",
    "    del GDP_over_time['Year'], GDP_over_time['GDP in billions of current dollars 1'], GDP_over_time['GDP in billions of chained 2009 dollars 1'] # deleting unwanted columns\n",
    "    GDP_over_time.index = list(range(len(GDP_over_time))) # reindexing from 0 to length of the datarfame\n",
    "    Quarterly_GDP = 0.0\n",
    "    GDP_Decline_Counter = 0\n",
    "    recession_start_time = None\n",
    "    for index, entry in enumerate(GDP_over_time['GDP in billions of current dollars 2']): # extracting recession bottom quarter\n",
    "        if entry > Quarterly_GDP:\n",
    "            Quarterly_GDP = entry\n",
    "        elif GDP_Decline_Counter==2:\n",
    "            Quarterly_GDP = entry\n",
    "            recession_start_time = GDP_over_time['Quarter'][int(index)-3]\n",
    "        else:\n",
    "            Quarterly_GDP = entry\n",
    "            GDP_Decline_Counter += 1\n",
    "    \n",
    "    recession_start_quarter = GDP_over_time[GDP_over_time['Quarter']==recession_start_time].index[0]\n",
    "    GDP_over_time_revised = GDP_over_time.loc[recession_start_quarter:]\n",
    "    GDP_over_time_revised.index = list(range(len(GDP_over_time_revised)))\n",
    "    \n",
    "    Quarterly_GDP = GDP_over_time_revised['GDP in billions of current dollars 2'][0]\n",
    "    GDP_Growth_Counter = 0\n",
    "    recession_end_time = 0\n",
    "    for index, entry in enumerate(GDP_over_time_revised['GDP in billions of current dollars 2']): # extracting recession bottom quarter\n",
    "        if GDP_Growth_Counter == 2:\n",
    "            Quarterly_GDP = entry\n",
    "            recession_end_time = GDP_over_time_revised['Quarter'][int(index-1)]\n",
    "            break\n",
    "        elif entry > Quarterly_GDP:\n",
    "            Quarterly_GDP = entry\n",
    "            GDP_Growth_Counter += 1\n",
    "        else:\n",
    "            Quarterly_GDP = entry\n",
    "\n",
    "    recession_end_quarter = GDP_over_time_revised[GDP_over_time_revised['Quarter']==recession_end_time].index[0]\n",
    "    GDP_over_time_revised = GDP_over_time_revised.loc[:recession_end_quarter]\n",
    "    argument_minimum = np.argmin(GDP_over_time_revised['GDP in billions of current dollars 2'], axis=0)[0]\n",
    "    return GDP_over_time_revised['Quarter'][argument_minimum] # returning recession bottom quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recession_bottom_quarter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_housing_data_to_mean_quarters_price():\n",
    "    '''Converts the housing data to quarters and returns it as mean \n",
    "    values in a dataframe.'''\n",
    "    housing_data = pd.read_csv(data_dir + 'City_Zhvi_AllHomes.csv') # read City_Zhvi_AllHomes.csv which contains median home sale prices and store it in a dataframe\n",
    "    housing_data.drop(housing_data.columns[6:51],axis=1, inplace=True) # frod unwanted year columns\n",
    "    \n",
    "    for index_2 in range(int(67)): # data preprocessing for renaming quarter columns\n",
    "        index_quarter = str(2000 + divmod(index_2, 4)[0])+\"q\"+str(((index_2)%4)+1)\n",
    "        housing_data[index_quarter] = 0\n",
    "    \n",
    "    housing_data_revised = housing_data.copy() # making a copy of the dataframe for help in renaming quarter columns\n",
    "    housing_data.drop(housing_data.columns[-67:],axis=1, inplace=True)\n",
    "    housing_data_revised.drop(housing_data_revised.columns[6:-67],axis=1,inplace=True)\n",
    "    \n",
    "    for index_2 in range(int(len(housing_data_revised.columns[6:]))): # data preprocessing for renaming quarter columns\n",
    "        housing_data_revised[housing_data_revised.columns[6:][index_2]] = housing_data[housing_data.columns[6+(index_2*3):6+(index_2*3)+3]].mean(axis=1)\n",
    "    \n",
    "    housing_data_revised = housing_data_revised.sort_values('State', ascending=True) # sort by state name in alphabetical order \n",
    "    housing_data_revised = housing_data_revised.replace(state_acronyms) # replace state values from two letter abbreviation to full state name according to aforementioned dictionary\n",
    "    housing_data_revised = housing_data_revised.set_index(['State','RegionName']) # setting a multi index of state and county\n",
    "    housing_data_revised.drop(housing_data_revised.columns[:4],axis=1,inplace=True) # droping unwanted columns\n",
    "    return housing_data_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2000q1</th>\n",
       "      <th>2000q2</th>\n",
       "      <th>2000q3</th>\n",
       "      <th>2000q4</th>\n",
       "      <th>2001q1</th>\n",
       "      <th>2001q2</th>\n",
       "      <th>2001q3</th>\n",
       "      <th>2001q4</th>\n",
       "      <th>2002q1</th>\n",
       "      <th>2002q2</th>\n",
       "      <th>...</th>\n",
       "      <th>2014q2</th>\n",
       "      <th>2014q3</th>\n",
       "      <th>2014q4</th>\n",
       "      <th>2015q1</th>\n",
       "      <th>2015q2</th>\n",
       "      <th>2015q3</th>\n",
       "      <th>2015q4</th>\n",
       "      <th>2016q1</th>\n",
       "      <th>2016q2</th>\n",
       "      <th>2016q3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th>RegionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Alaska</th>\n",
       "      <th>Kenai</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>199466.666667</td>\n",
       "      <td>197966.666667</td>\n",
       "      <td>194600.000000</td>\n",
       "      <td>198500.000000</td>\n",
       "      <td>202700.000000</td>\n",
       "      <td>204766.666667</td>\n",
       "      <td>206433.333333</td>\n",
       "      <td>209633.333333</td>\n",
       "      <td>211666.666667</td>\n",
       "      <td>211950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ketchikan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>196866.666667</td>\n",
       "      <td>197100.000000</td>\n",
       "      <td>197700.000000</td>\n",
       "      <td>198033.333333</td>\n",
       "      <td>196300.000000</td>\n",
       "      <td>196533.333333</td>\n",
       "      <td>196633.333333</td>\n",
       "      <td>195600.000000</td>\n",
       "      <td>205300.000000</td>\n",
       "      <td>206450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anchor Point</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>157800.000000</td>\n",
       "      <td>158000.000000</td>\n",
       "      <td>155566.666667</td>\n",
       "      <td>154266.666667</td>\n",
       "      <td>154100.000000</td>\n",
       "      <td>153966.666667</td>\n",
       "      <td>154766.666667</td>\n",
       "      <td>156800.000000</td>\n",
       "      <td>161533.333333</td>\n",
       "      <td>161200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anchorage</th>\n",
       "      <td>174633.333333</td>\n",
       "      <td>175266.666667</td>\n",
       "      <td>179566.666667</td>\n",
       "      <td>182833.333333</td>\n",
       "      <td>182766.666667</td>\n",
       "      <td>183933.333333</td>\n",
       "      <td>188566.666667</td>\n",
       "      <td>191866.666667</td>\n",
       "      <td>193966.666667</td>\n",
       "      <td>196700.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>278200.000000</td>\n",
       "      <td>280766.666667</td>\n",
       "      <td>281700.000000</td>\n",
       "      <td>284166.666667</td>\n",
       "      <td>287166.666667</td>\n",
       "      <td>290233.333333</td>\n",
       "      <td>291700.000000</td>\n",
       "      <td>293700.000000</td>\n",
       "      <td>294833.333333</td>\n",
       "      <td>294600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Palmer</th>\n",
       "      <td>168766.666667</td>\n",
       "      <td>170566.666667</td>\n",
       "      <td>174233.333333</td>\n",
       "      <td>175533.333333</td>\n",
       "      <td>169466.666667</td>\n",
       "      <td>163233.333333</td>\n",
       "      <td>166933.333333</td>\n",
       "      <td>168933.333333</td>\n",
       "      <td>165300.000000</td>\n",
       "      <td>164800.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>225566.666667</td>\n",
       "      <td>228766.666667</td>\n",
       "      <td>229366.666667</td>\n",
       "      <td>234466.666667</td>\n",
       "      <td>237800.000000</td>\n",
       "      <td>239900.000000</td>\n",
       "      <td>241666.666667</td>\n",
       "      <td>246400.000000</td>\n",
       "      <td>245733.333333</td>\n",
       "      <td>244750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seward</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>194600.000000</td>\n",
       "      <td>190533.333333</td>\n",
       "      <td>186600.000000</td>\n",
       "      <td>186800.000000</td>\n",
       "      <td>185533.333333</td>\n",
       "      <td>186433.333333</td>\n",
       "      <td>191000.000000</td>\n",
       "      <td>203600.000000</td>\n",
       "      <td>214666.666667</td>\n",
       "      <td>214950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kodiak</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>256400.000000</td>\n",
       "      <td>256966.666667</td>\n",
       "      <td>257933.333333</td>\n",
       "      <td>258566.666667</td>\n",
       "      <td>256133.333333</td>\n",
       "      <td>256400.000000</td>\n",
       "      <td>256800.000000</td>\n",
       "      <td>255233.333333</td>\n",
       "      <td>270166.666667</td>\n",
       "      <td>271250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tanaina</th>\n",
       "      <td>160500.000000</td>\n",
       "      <td>163500.000000</td>\n",
       "      <td>166366.666667</td>\n",
       "      <td>168033.333333</td>\n",
       "      <td>158766.666667</td>\n",
       "      <td>149200.000000</td>\n",
       "      <td>150966.666667</td>\n",
       "      <td>151600.000000</td>\n",
       "      <td>153033.333333</td>\n",
       "      <td>156100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208900.000000</td>\n",
       "      <td>211000.000000</td>\n",
       "      <td>212466.666667</td>\n",
       "      <td>215200.000000</td>\n",
       "      <td>215633.333333</td>\n",
       "      <td>217700.000000</td>\n",
       "      <td>218800.000000</td>\n",
       "      <td>223100.000000</td>\n",
       "      <td>225200.000000</td>\n",
       "      <td>225500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lakes</th>\n",
       "      <td>172733.333333</td>\n",
       "      <td>174133.333333</td>\n",
       "      <td>177266.666667</td>\n",
       "      <td>179133.333333</td>\n",
       "      <td>172400.000000</td>\n",
       "      <td>166000.000000</td>\n",
       "      <td>169500.000000</td>\n",
       "      <td>170766.666667</td>\n",
       "      <td>168766.666667</td>\n",
       "      <td>169900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>228200.000000</td>\n",
       "      <td>233966.666667</td>\n",
       "      <td>236966.666667</td>\n",
       "      <td>239833.333333</td>\n",
       "      <td>240033.333333</td>\n",
       "      <td>242733.333333</td>\n",
       "      <td>244500.000000</td>\n",
       "      <td>250100.000000</td>\n",
       "      <td>250266.666667</td>\n",
       "      <td>250550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Pole</th>\n",
       "      <td>157466.666667</td>\n",
       "      <td>159100.000000</td>\n",
       "      <td>162133.333333</td>\n",
       "      <td>165500.000000</td>\n",
       "      <td>159433.333333</td>\n",
       "      <td>149766.666667</td>\n",
       "      <td>151266.666667</td>\n",
       "      <td>151000.000000</td>\n",
       "      <td>150033.333333</td>\n",
       "      <td>151866.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>199933.333333</td>\n",
       "      <td>199400.000000</td>\n",
       "      <td>197366.666667</td>\n",
       "      <td>199066.666667</td>\n",
       "      <td>200300.000000</td>\n",
       "      <td>204100.000000</td>\n",
       "      <td>206400.000000</td>\n",
       "      <td>206633.333333</td>\n",
       "      <td>208166.666667</td>\n",
       "      <td>209300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            2000q1         2000q2         2000q3  \\\n",
       "State  RegionName                                                  \n",
       "Alaska Kenai                   NaN            NaN            NaN   \n",
       "       Ketchikan               NaN            NaN            NaN   \n",
       "       Anchor Point            NaN            NaN            NaN   \n",
       "       Anchorage     174633.333333  175266.666667  179566.666667   \n",
       "       Palmer        168766.666667  170566.666667  174233.333333   \n",
       "       Seward                  NaN            NaN            NaN   \n",
       "       Kodiak                  NaN            NaN            NaN   \n",
       "       Tanaina       160500.000000  163500.000000  166366.666667   \n",
       "       Lakes         172733.333333  174133.333333  177266.666667   \n",
       "       North Pole    157466.666667  159100.000000  162133.333333   \n",
       "\n",
       "                            2000q4         2001q1         2001q2  \\\n",
       "State  RegionName                                                  \n",
       "Alaska Kenai                   NaN            NaN            NaN   \n",
       "       Ketchikan               NaN            NaN            NaN   \n",
       "       Anchor Point            NaN            NaN            NaN   \n",
       "       Anchorage     182833.333333  182766.666667  183933.333333   \n",
       "       Palmer        175533.333333  169466.666667  163233.333333   \n",
       "       Seward                  NaN            NaN            NaN   \n",
       "       Kodiak                  NaN            NaN            NaN   \n",
       "       Tanaina       168033.333333  158766.666667  149200.000000   \n",
       "       Lakes         179133.333333  172400.000000  166000.000000   \n",
       "       North Pole    165500.000000  159433.333333  149766.666667   \n",
       "\n",
       "                            2001q3         2001q4         2002q1  \\\n",
       "State  RegionName                                                  \n",
       "Alaska Kenai                   NaN            NaN            NaN   \n",
       "       Ketchikan               NaN            NaN            NaN   \n",
       "       Anchor Point            NaN            NaN            NaN   \n",
       "       Anchorage     188566.666667  191866.666667  193966.666667   \n",
       "       Palmer        166933.333333  168933.333333  165300.000000   \n",
       "       Seward                  NaN            NaN            NaN   \n",
       "       Kodiak                  NaN            NaN            NaN   \n",
       "       Tanaina       150966.666667  151600.000000  153033.333333   \n",
       "       Lakes         169500.000000  170766.666667  168766.666667   \n",
       "       North Pole    151266.666667  151000.000000  150033.333333   \n",
       "\n",
       "                            2002q2  ...         2014q2         2014q3  \\\n",
       "State  RegionName                   ...                                 \n",
       "Alaska Kenai                   NaN  ...  199466.666667  197966.666667   \n",
       "       Ketchikan               NaN  ...  196866.666667  197100.000000   \n",
       "       Anchor Point            NaN  ...  157800.000000  158000.000000   \n",
       "       Anchorage     196700.000000  ...  278200.000000  280766.666667   \n",
       "       Palmer        164800.000000  ...  225566.666667  228766.666667   \n",
       "       Seward                  NaN  ...  194600.000000  190533.333333   \n",
       "       Kodiak                  NaN  ...  256400.000000  256966.666667   \n",
       "       Tanaina       156100.000000  ...  208900.000000  211000.000000   \n",
       "       Lakes         169900.000000  ...  228200.000000  233966.666667   \n",
       "       North Pole    151866.666667  ...  199933.333333  199400.000000   \n",
       "\n",
       "                            2014q4         2015q1         2015q2  \\\n",
       "State  RegionName                                                  \n",
       "Alaska Kenai         194600.000000  198500.000000  202700.000000   \n",
       "       Ketchikan     197700.000000  198033.333333  196300.000000   \n",
       "       Anchor Point  155566.666667  154266.666667  154100.000000   \n",
       "       Anchorage     281700.000000  284166.666667  287166.666667   \n",
       "       Palmer        229366.666667  234466.666667  237800.000000   \n",
       "       Seward        186600.000000  186800.000000  185533.333333   \n",
       "       Kodiak        257933.333333  258566.666667  256133.333333   \n",
       "       Tanaina       212466.666667  215200.000000  215633.333333   \n",
       "       Lakes         236966.666667  239833.333333  240033.333333   \n",
       "       North Pole    197366.666667  199066.666667  200300.000000   \n",
       "\n",
       "                            2015q3         2015q4         2016q1  \\\n",
       "State  RegionName                                                  \n",
       "Alaska Kenai         204766.666667  206433.333333  209633.333333   \n",
       "       Ketchikan     196533.333333  196633.333333  195600.000000   \n",
       "       Anchor Point  153966.666667  154766.666667  156800.000000   \n",
       "       Anchorage     290233.333333  291700.000000  293700.000000   \n",
       "       Palmer        239900.000000  241666.666667  246400.000000   \n",
       "       Seward        186433.333333  191000.000000  203600.000000   \n",
       "       Kodiak        256400.000000  256800.000000  255233.333333   \n",
       "       Tanaina       217700.000000  218800.000000  223100.000000   \n",
       "       Lakes         242733.333333  244500.000000  250100.000000   \n",
       "       North Pole    204100.000000  206400.000000  206633.333333   \n",
       "\n",
       "                            2016q2    2016q3  \n",
       "State  RegionName                             \n",
       "Alaska Kenai         211666.666667  211950.0  \n",
       "       Ketchikan     205300.000000  206450.0  \n",
       "       Anchor Point  161533.333333  161200.0  \n",
       "       Anchorage     294833.333333  294600.0  \n",
       "       Palmer        245733.333333  244750.0  \n",
       "       Seward        214666.666667  214950.0  \n",
       "       Kodiak        270166.666667  271250.0  \n",
       "       Tanaina       225200.000000  225500.0  \n",
       "       Lakes         250266.666667  250550.0  \n",
       "       North Pole    208166.666667  209300.0  \n",
       "\n",
       "[10 rows x 67 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_housing_data_to_mean_quarters_price().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest():\n",
    "    '''First the new data showing the decline or growth of housing prices between the recession start \n",
    "    and recession botton. Then a ttest is ran to compare the university town values to the non-university\n",
    "    town values, return whether the alternative hypotheses is true or not. The p-value of confidence is \n",
    "    then computed.\n",
    "    \n",
    "    A tuple (different, p, better) is returned where different=True if the t-test is\n",
    "    True at a p<0.01 (we reject the null hypothesis), or different=False if \n",
    "    otherwise (we cannot reject the null hypothesis). The variable p should\n",
    "    be equal to the exact p value returned from scipy.stats.ttest_ind(). The\n",
    "    value for better should be either \"university town\" or \"non-university town\"\n",
    "    depending on which has a lower mean price ratio (which is equivilent to a\n",
    "    reduced market loss).'''\n",
    "    \n",
    "    housing_data_revised = convert_housing_data_to_mean_quarters_price() # obtain mean quarterly housing values from the above function\n",
    "    housing_data_revised = housing_data_revised[housing_data_revised.columns[33:-29]] # extract mean quarterly housing values from only the recession period\n",
    "    \n",
    "    university_towns = get_university_towns_list() #obtain list of university towns form the above function\n",
    "    university_towns = university_towns.sort_values('State', ascending=True) # sort university towns by state name in alphabetical order\n",
    "    university_towns = university_towns.set_index(['State','RegionName']) # set state name and county as multi-index\n",
    "    \n",
    "    university_town_values = pd.merge(university_towns, housing_data_revised, how='inner', left_index=True, right_index=True) # obtain mean quarterly housing values in university towns\n",
    "    non_university_town_values = housing_data_revised[~housing_data_revised.index.isin(university_town_values.index)] # obtain mean quarterly values of housing in non-university towns\n",
    "\n",
    "    university_town_mean_ratio = ((university_town_values['2008q2'])/(university_town_values['2009q2'])).mean() # obtain ratio of mean quarterly housing values in recession start to recession bottom in university towns\n",
    "    non_university_town_mean_ratio = ((non_university_town_values['2008q2'])/(non_university_town_values['2009q2'])).mean() # obtain ratio of mean quarterly housing values in recession start to recession bottom in non-university towns\n",
    "    \n",
    "    from scipy import stats\n",
    "\n",
    "    p = stats.ttest_ind((university_town_values['2008q2'])/(university_town_values['2009q2']), (non_university_town_values['2008q2'])/(non_university_town_values['2009q2']), nan_policy='omit')[1] # run t-test on the above ratios of university and non-university towns\n",
    "    different = p<0.01\n",
    "    better = \"university town\" if (university_town_mean_ratio < non_university_town_mean_ratio) else \"non-university town\"\n",
    "    \n",
    "    return (different, p, better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.002724063704753125, 'university town')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-data-analysis",
   "graded_item_id": "Il9Fx",
   "launcher_item_id": "TeDW0",
   "part_id": "WGlun"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
